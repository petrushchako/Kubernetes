# Installing and Configuring Kubernetes

### Table of contents
  - Module Overview
  - Installation Considerations
  - Installation Methods
  - Installation Requirements
  - Understanding Cluster Networking Ports
  - Getting Kubernetes
  - Building your Own Cluster
  - Installing Kubernetes on VMs
  - Lab Environment Overview
  - `Demo`: Installing and configuring containerd
  - `Demo`: Installing and configuring Kubernetes Packages
  - Bootstrapping a Cluster with kubeadm
  - Understanding the Certificate Authority Role in your Cluster
  - kubeadm Created kubeconfig Files and Static Pod Manifests
  - Pod Networking Fundamentals
  - Creating a Cluster Control Plane Node and Adding a Node
  - `Demo`: Creating a Cluster Control Plane Node
  - `Demo`: Adding a Node to Your Cluster
  - Managed Cloud Deployment Scenarious: AKS, EKS, and GKE
  - `Demo`: Creating a Cluster in the Cloud with Azure Kubernetes Service
  - Module Summary and what is next


<br><br><br>


## Module Overview
- Installation Considerations
- Installation Overview
- Getting Kubernetes
- Installing a Cluster with `kubeadm`
- Creating a Cluster in the Cloud



<br><br><br>



## Installation Considerations
When setting up a Kubernetes cluster, several factors must be considered, including deployment location, networking, scalability, and high availability.  

### **Deployment Options**  
1. **Cloud Deployments**  
   - **Infrastructure as a Service (IaaS):**  
     - Deploy Kubernetes on virtual machines (VMs) in the cloud.  
     - The cloud provider manages networking and infrastructure, but you manage OS, patches, and Kubernetes itself.  
   - **Platform as a Service (PaaS):**  
     - Fully managed Kubernetes services (e.g., AWS EKS, Azure AKS, Google GKE).  
     - No need to manage infrastructure, but limited control over Kubernetes versions and features.  
2. **On-Premises Deployments**  
   - **Bare Metal:**  
     - Kubernetes is installed on physical machines, requiring full infrastructure management.  
   - **Virtual Machines (VMs):**  
     - Kubernetes runs on VMs within an on-premises data center, offering a common alternative to bare metal.  

### **Key Considerations Before Installation**  
1. **Networking**  
   - Pods must communicate across nodes without **NAT**.  
   - Choose between **overlay networks** (e.g., Flannel, Calico) or direct Layer 2/3 routing.  
   - Avoid **IP range conflicts** between Kubernetes networking and the existing infrastructure.  
2. **Scalability**  
   - Ensure sufficient **CPU, memory, and storage** across nodes.  
   - Plan capacity to handle **node failures** while maintaining workload availability.  
3. **High Availability & Disaster Recovery**  
   - A **single control plane node** is a single point of failure.  
   - Use **multiple control plane nodes** for redundancy (HA setup).  
   - Maintain **etcd backups** for disaster recovery.  



<br><br><br>



## Installation methods
There are multiple ways to install Kubernetes, each suited for different use cases such as development, testing, and production deployments.  

### **1. Desktop Installations**  
- Ideal for **development and learning**.  
- **Docker Desktop** includes a lightweight Kubernetes cluster:  
  - **Mac:** Kubernetes is integrated into Docker Desktop for easy setup.  
  - **Windows:** Kubernetes support is also available within Docker Desktop.  

### **2. kubeadm (Standard Installation Method)**  
- **kubeadm** is the **preferred tool** for bootstrapping a Kubernetes cluster.  
- Allows for quick setup while maintaining flexibility.  
- This course will focus on installing Kubernetes **on virtual machines using kubeadm**.  

### **3. Cloud-Based Installations**  
- Deploy Kubernetes in the cloud using:  
  - **Infrastructure as a Service (IaaS):** Install Kubernetes on cloud VMs.  
  - **Platform as a Service (PaaS):** Use managed Kubernetes services (AWS EKS, Azure AKS, Google GKE).  
- Cloud options offer scalability and ease of management but may have restrictions on Kubernetes versions and configurations.  

### **Installation Focus in This Course**  
- Kubernetes will be installed on **on-premises virtual machines using kubeadm**.  
- Later, cloud-based Kubernetes deployments will also be explored.



<br><br><br>


## Installation Requirements

When installing Kubernetes on **bare metal** or **virtual machines**, you must meet specific system requirements to ensure a stable cluster.  


|**System Requirements**|**Container Runtime**|**Networking**|
|---|---|---|
|Linux - Ubuntu/RHEL|Container Runtime Interface (CRI)|Connectivity between all Nodes|
|2 CPUs|containerd|Unique hostname|
|2GB RAM|Docker (Depracated 1.20)|Unique MAC address|
|Swap disabled|CRI-O||

<br><br><br>


## Cluster Network Ports

![](img/2.3.Architecture-Node.png)

### Control Plane Node
|**Component**|**Ports (tcp)**|**Used By**|
|---|---|---|
|API|6443|All|
|etcd|2379-2380|API/etcd|
|Scheduler|10251|Self|
|Control Manager|10252|Self|
|Kubelet|10250|Control Plane|


### Worker Node
|**Component**|**Ports (tcp)**|**Used By**|
|---|---|---|
|Kubelet|10250|Control Plane|
|NodePort|30000-32767|All|



<br><br><br>



## Getting Kubernetes
Kubernetes is an open-source project actively maintained on GitHub. You can find the latest source code and contribute to the project at.

> [GitHub Repository: Kubernetes](https://github.com/kubernetes/kubernetes)  

This repository contains:  
- **Source Code**: The core Kubernetes components.  
- **Documentation**: In-depth technical details about Kubernetes internals.  
- **Community Contributions**: Active discussions and updates from contributors worldwide.  

### **Methods to Install Kubernetes**  
For most production environments and this course, we will install Kubernetes from **Linux distribution package repositories**, using:  
- **`apt` (Advanced Packaging Tool)** – For Debian-based distributions like **Ubuntu**.  
- **`yum` (Yellowdog Updater, Modified)** – For RHEL-based distributions like **CentOS**.  

In this course, we will be using **Ubuntu** and installing Kubernetes from an **APT repository**.



<br><br><br>



## Build Your Own Cluster
To set up a Kubernetes cluster, follow these key steps:  

1. **Install and Configure a Container Runtime & Kubernetes Packages**  
We will be using **containerd** as our container runtime in this course. The required packages include:  
   - **Container Runtime**: `containerd` (or alternatively, Docker).  
   - **Kubelet**: The agent running on each node to manage workloads.  
   - **Kubeadm**: A CLI tool to bootstrap the Kubernetes cluster.  
   - **Kubectl**: The command-line tool to interact with the cluster.  

These packages should be installed on **all nodes** (both control-plane and worker nodes).  

1. **Bootstrap the First Control-Plane Node**  
Once the packages are installed, we use **kubeadm** to initialize the cluster, setting up essential components:  
   - **API Server**  
   - **Controller Manager**  
   - **etcd (Key-Value Store)**  
   - **Scheduler**  

1. **Configure Pod Networking**  
To enable communication between pods across nodes, we will set up an **overlay network**.  

1. **Join Worker Nodes to the Cluster**  
Once networking is configured, additional worker nodes can be added to the cluster using `kubeadm join`.  



<br><br><br>


## Understanding the Certificate Authority's Role in Your Cluster
In a Kubernetes cluster, **security and trust** between components are vital. This is where the **Certificate Authority (CA)** plays a critical role. The CA is responsible for issuing and verifying **TLS certificates** that secure communication between the different cluster components, such as the API server, kubelet, controller manager, scheduler, and etcd.

### Why Certificates Matter

Kubernetes uses **mutual TLS (mTLS)** to authenticate and encrypt communication between components. Each component in the control plane and each kubelet running on a node presents a certificate that proves its identity.

Without valid certificates signed by a trusted CA:
- Nodes cannot register with the control plane.
- The kubelet cannot securely talk to the API server.
- Communication with etcd (which stores all cluster data) would be insecure.

### What the CA Does

The CA:
- Issues certificates to cluster components (e.g., `kube-apiserver`, `kubelet`, `etcd`)
- Signs Certificate Signing Requests (CSRs)
- Verifies certificates during secure communication

This allows Kubernetes to **trust** that the component it’s talking to is legitimate and that the data exchanged is secure.

### How Certificates Are Managed

When using `kubeadm` to create a cluster, it will:
- Generate a self-signed CA by default (stored in `/etc/kubernetes/pki/ca.crt`)
- Use this CA to issue certificates to all required components
- Optionally allow you to provide your own external CA if needed

The CA certificate itself is distributed to clients and nodes so they can verify server identities.

### Important CA Files

Typical file locations on a control plane node:
- `/etc/kubernetes/pki/ca.crt` – The public CA certificate (used to verify)
- `/etc/kubernetes/pki/ca.key` – The private key for signing (keep secure!)



<br><br><br>


## `kubeadm` Created kubeconfig Files and Static Pod Manifests
When you run `kubeadm init`, one of its most critical tasks is to generate configuration files and pod definitions that bootstrap the entire Kubernetes control plane. Two important outputs of this process are:

- **Kubeconfig files**
- **Static Pod manifests**

<br>

### Kubeconfig Files: API Access Configurations
A **kubeconfig** file is a YAML configuration that defines **how clients (users or system components) connect to the Kubernetes API server**.

Each kubeconfig file includes:
- The **API server's address** (IP or DNS)
- A **CA certificate** to verify the API server's identity
- A **client certificate and key** for secure authentication

#### Location of Kubeconfig Files
These files are created by `kubeadm` and stored in:

```
/etc/kubernetes/
```

#### Key kubeconfig Files Generated by `kubeadm`

| File | Purpose |
|------|--------|
| `admin.conf` | Used by cluster administrators for full access via `kubectl` |
| `controller-manager.conf` | Used by the kube-controller-manager to talk to the API server |
| `scheduler.conf` | Used by the kube-scheduler to talk to the API server |
| `kubelet.conf` | Used by the kubelet on the control-plane node |
| `super-admin.conf` | (Rare) Emergency config that bypasses RBAC, useful for recovery scenarios |

💡 You can copy `admin.conf` to your local system as `~/.kube/config` to interact with the cluster via `kubectl`.


<br>


### Static Pod Manifests: Bootstrapping the Control Plane
Static Pods are pods managed directly by the **kubelet**, not by the API server. This is key during initial cluster bootstrap—when the API server isn’t even running yet.

#### Location of Static Pod Manifests

```
/etc/kubernetes/manifests/
```

These YAML manifest files describe core Kubernetes control plane components as static pods. The kubelet monitors this directory and automatically starts these pods when it detects their manifests.

#### Control Plane Components as Static Pods
- `etcd.yaml` – Local cluster database
- `kube-apiserver.yaml` – Central API server
- `kube-controller-manager.yaml` – Cluster state manager
- `kube-scheduler.yaml` – Workload assignment

#### How They Work
1. On boot, **systemd** starts the kubelet service.
2. The kubelet scans `/etc/kubernetes/manifests/` for pod definitions.
3. It launches the components defined there, even if the API server is not available yet.

This allows the cluster to self-initialize and recover from reboots—ensuring core services always start, even without a fully functioning cluster at first.


<br><br><br>


## Pod Networking Fundamentals

Before standing up a Kubernetes cluster, it's essential to understand and configure **pod networking** according to the Kubernetes networking model.

### Key Requirements of Kubernetes Networking

- Each pod must have a **unique, un-NATed IP address**.
- **Full IP reachability** must be maintained:
  - Between all pods across nodes.
  - Between all nodes and all pods.

### Network Design Options

#### 1. **Direct Routing**
- Configure the underlying network to ensure end-to-end IP reachability without NAT.
- Uses **real IP addresses**.
- Often **not feasible** in:
  - Cloud environments.
  - Scenarios without control over physical networking.

#### 2. **Overlay Networking (Software-Defined Networking)**
- Provides a **virtual Layer 3 network** that allows all pods to communicate.
- Achieves networking through:
  - **Tunneling**
  - **Encapsulation of IP packets**
- Ensures communication appears native, even though packets are wrapped and routed by the overlay system.

### Common Overlay Network Solutions
| Overlay Network | Description |
|-----------------|-------------|
| **Flannel**     | Simple to set up, lightweight, commonly used with kubeadm. |
| **Calico**      | Powerful, supports network policies, used in demo setups. |
| **Weave Net**   | Flexible, supports encryption and automatic peer discovery. |

> In the demo, **Calico** will be used to manage IP address allocation for pods.

### Important Considerations

- The **pod CIDR range** must **not overlap** with any existing network infrastructure.
- Coordinate with your **network engineering team** to determine a valid, conflict-free IP range.

### Further Learning

- For an in-depth understanding of overlay networks and additional networking solutions, refer to the course link provided.
- Future courses in this series will cover **Kubernetes networking** in more depth.


<br><br><br>


## Creating a Cluster Control Plane Node and Adding a Node
After completing the necessary setup, the next major step is to bootstrap your control plane node using `kubeadm`. Here's how to proceed:

### 1. Download the Pod Network Manifest
The first requirement is to deploy a pod network so that pods can communicate across the cluster. You’ll need to download a YAML manifest for the pod network plugin, such as Calico:

```bash
wget https://docs.projectcalico.org/manifests/calico.yaml
```

Inside the `calico.yaml` file, there’s a section where you can define the Pod Network CIDR. This should match the CIDR specified during cluster initialization.

> If this URL ever changes, the updated link will be provided in the course materials.

<br>

### 2. Run `kubeadm init`
Kick off the initialization of the Kubernetes control plane by running:

```bash
sudo kubeadm init --pod-network-cidr=192.168.0.0/16
```

This process:
- Creates and configures the control plane components (API server, controller manager, scheduler, etcd).
- Prints out the join command and token needed to add worker nodes to the cluster.
- Provides instructions to set up the kubeconfig file for the administrative user.
